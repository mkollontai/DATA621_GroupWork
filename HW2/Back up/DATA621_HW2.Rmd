---
title: "DATA621_Homework2"
author: "Don Padmaperuma"
date: "10/5/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
In this homework assignment, we will work through various classification metrics. we will create
functions in R to carry out the various calculations. We will also investigate some functions in packages that will let
us obtain the equivalent results. Finally, we will create graphical output that also can be used to evaluate the
output of classification models, such as binary logistic regression.  
Reference:   
  *http://www.saedsayad.com/model_evaluation_c.htm*   
  *Chapter 11- Measuring Performance in Classification Models*  
  

## Instructions  
>1. Download the classification output data set

```{r}
class_df <- read.csv("https://raw.githubusercontent.com/gpadmaperuma/DATA621/main/classification-output-data%20(2).csv", header = TRUE)
head(class_df)
```

>2. Get to know the data set  

```{r}
summary(class_df)
```
>2.1 Confusion Matrix

```{r}
#confusion matrix

conf_matrix <- with(class_df, table("Actual" = class, "Predicted" = scored.class))
addmargins(conf_matrix)
```
Confusion matrix is all about number of correct and incorrect predictions compared to actual outcomes in the dataset. Rows in this matrix represent the number of actual classes (0 and 1) of the data set and columns represent the number of predicted classes (0 and 1) of the dataset.   

>3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the accuracy of the predictions.   

$𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = (\frac{𝑇𝑃 + 𝑇𝑁}{𝑇𝑃 + 𝐹𝑃 + 𝑇𝑁 + 𝐹𝑁})$  


```{r}
# Accuracy 
accuracy <- function(df)
{
  cmat <- table(df$class, df$scored.class)
  acc <- (cmat[2,2]+cmat[1,1])/sum(cmat)
  return(acc)
}

accuracy(class_df)
```

>4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.  

$𝐴𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛 𝐸𝑟𝑟𝑜𝑟 𝑅𝑎𝑡e = (\frac{𝐹𝑃 + 𝐹𝑁}{𝑇𝑃 + 𝐹𝑃 + 𝑇𝑁 + 𝐹𝑁})$  
      
```{r}
classification_error <- function(df){
  
  cmat <- table(df$class, df$scored.class)
  error <- (cmat[1,2]+cmat[2,1])/sum(cmat)
  return(error)
}
classification_error(class_df)
```
```{r}
# Check to see if accuracy and classification error adds up to 1
sum <- accuracy(class_df)+classification_error(class_df)
sum
```
>5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the precision of the predictions.  

$𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = \frac{𝑇𝑃}{𝑇𝑃 + 𝐹𝑃}$  

```{r}
precision <- function(df){
  cmat <- table(df$class, df$scored.class)
  prec <- (cmat[2,2])/(cmat[2,2]+cmat[1,2])
  return(prec)
}
precision(class_df)
```
>6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the sensitivity of the predictions. Sensitivity is also known as recall.    

$𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 =\frac{𝑇𝑃}{𝑇𝑃 + 𝐹𝑁}$  

```{r}
sensitivity <- function(df){
  cmat <- table(df$class, df$scored.class)
  sens <- (cmat[2,2])/(cmat[2,2]+cmat[2,1])
  return(sens)
}
sensitivity(class_df)
```

>7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the specificity of the predictions    

$𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 =\frac{𝑇𝑁}{𝑇𝑁 + 𝐹𝑃}$  

```{r}
specificity <- function(df){
  cmat <- table(df$class, df$scored.class)
  spec <- (cmat[1,1])/(cmat[1,1]+cmat[1,2])
  return(spec)
}
specificity(class_df)
```

>8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions.  

$𝐹1 𝑆𝑐𝑜𝑟𝑒 = \frac{2 × 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦} {𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦}$  


```{r}
f1_score <- function(df){
  p <- precision(df)
  s <- sensitivity(df)
  (2*p*s)/(p+s)
}
f1_score(class_df)
```

>9. Before we move on, let’s consider a question that was asked: What are the bounds on the F1 score? Show
that the F1 score will always be between 0 and 1. $(Hint: If 0 < 𝑎 < 1 and 0 < 𝑏 < 1 then 𝑎𝑏 < 𝑎.)$  

F1 score is the harmonic mean of Precision and Sensitivity (Recall). This takes both false positives and false negatives into account. The highest possible value of F1 score is *1*, and the lowest possible F1 score is *0*. 

```{r}
# assume a is precision and b is sensitivity  

a <- runif(100, min=0, max=1)
b <- runif(100, min=0, max=1)
f1 <- (2*a*b)/(a+b)
summary(f1)
```












