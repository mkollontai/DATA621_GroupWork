---
title: "DATA621_Homework2"
author: "Don Padmaperuma"
date: "10/5/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
In this homework assignment, we will work through various classification metrics. we will create
functions in R to carry out the various calculations. We will also investigate some functions in packages that will let
us obtain the equivalent results. Finally, we will create graphical output that also can be used to evaluate the
output of classification models, such as binary logistic regression.  
Reference:   
  *http://www.saedsayad.com/model_evaluation_c.htm*   
  *Chapter 11- Measuring Performance in Classification Models*  
  

## Instructions  
>1. Download the classification output data set

```{r}
class_df <- read.csv("https://raw.githubusercontent.com/gpadmaperuma/DATA621/main/classification-output-data%20(2).csv", header = TRUE)
head(class_df)
```

>2. Get to know the data set  

```{r}
summary(class_df)
```
>2.1 Confusion Matrix

```{r}
#confusion matrix

conf_matrix <- with(class_df, table("Actual" = class, "Predicted" = scored.class))
addmargins(conf_matrix)
```
Confusion matrix is all about number of correct and incorrect predictions compared to actual outcomes in the dataset. Rows in this matrix represent the number of actual classes (0 and 1) of the data set and columns represent the number of predicted classes (0 and 1) of the dataset.   

>3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the accuracy of the predictions.   

$ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = (\frac{ğ‘‡ğ‘ƒ + ğ‘‡ğ‘}{ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘})$  


```{r}
# Accuracy 
accuracy <- function(df)
{
  cmat <- table(df$class, df$scored.class)
  acc <- (cmat[2,2]+cmat[1,1])/sum(cmat)
  return(acc)
}

accuracy(class_df)
```

>4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.  

$ğ´ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ ğ‘…ğ‘ğ‘¡e = (\frac{ğ¹ğ‘ƒ + ğ¹ğ‘}{ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘})$  
      
```{r}
classification_error <- function(df){
  
  cmat <- table(df$class, df$scored.class)
  error <- (cmat[1,2]+cmat[2,1])/sum(cmat)
  return(error)
}
classification_error(class_df)
```
```{r}
# Check to see if accuracy and classification error adds up to 1
sum <- accuracy(class_df)+classification_error(class_df)
sum
```
>5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the precision of the predictions.  

$ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = \frac{ğ‘‡ğ‘ƒ}{ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ}$  

```{r}
precision <- function(df){
  cmat <- table(df$class, df$scored.class)
  prec <- (cmat[2,2])/(cmat[2,2]+cmat[1,2])
  return(prec)
}
precision(class_df)
```
>6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the sensitivity of the predictions. Sensitivity is also known as recall.    

$ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ =\frac{ğ‘‡ğ‘ƒ}{ğ‘‡ğ‘ƒ + ğ¹ğ‘}$  

```{r}
sensitivity <- function(df){
  cmat <- table(df$class, df$scored.class)
  sens <- (cmat[2,2])/(cmat[2,2]+cmat[2,1])
  return(sens)
}
sensitivity(class_df)
```

>7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the specificity of the predictions    

$ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ =\frac{ğ‘‡ğ‘}{ğ‘‡ğ‘ + ğ¹ğ‘ƒ}$  

```{r}
specificity <- function(df){
  cmat <- table(df$class, df$scored.class)
  spec <- (cmat[1,1])/(cmat[1,1]+cmat[1,2])
  return(spec)
}
specificity(class_df)
```

>8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions.  

$ğ¹1 ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = \frac{2 Ã— ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã— ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦} {ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦}$  


```{r}
f1_score <- function(df){
  p <- precision(df)
  s <- sensitivity(df)
  (2*p*s)/(p+s)
}
f1_score(class_df)
```

>9. Before we move on, letâ€™s consider a question that was asked: What are the bounds on the F1 score? Show
that the F1 score will always be between 0 and 1. $(Hint: If 0 < ğ‘ < 1 and 0 < ğ‘ < 1 then ğ‘ğ‘ < ğ‘.)$  

F1 score is the harmonic mean of Precision and Sensitivity (Recall). This takes both false positives and false negatives into account. The highest possible value of F1 score is *1*, and the lowest possible F1 score is *0*. 

```{r}
# assume a is precision and b is sensitivity  

a <- runif(100, min=0, max=1)
b <- runif(100, min=0, max=1)
f1 <- (2*a*b)/(a+b)
summary(f1)
```












